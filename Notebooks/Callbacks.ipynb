{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Callbacks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEEXs3PquCaY"
      },
      "source": [
        "# Using Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukSntKrOteWi"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84gdzl1pu0jY",
        "outputId": "70798f52-ceed-47aa-bad6-28231d1a65af"
      },
      "source": [
        "# loading and splitting the dataset into train, valid and test set\r\n",
        "housing = fetch_california_housing()\r\n",
        "\r\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEleF237u2dW"
      },
      "source": [
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_valid = scaler.fit_transform(X_valid)\r\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUb2EcKgu4Sl"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "                                 keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\r\n",
        "                                 keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "                                 keras.layers.Dense(1)\r\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzWecD4NuYWb"
      },
      "source": [
        "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gZgllg9uGEe"
      },
      "source": [
        "If training lasts several hourse, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if computer crashes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPqG3doAwLjC"
      },
      "source": [
        "Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67kgPWvEvME9",
        "outputId": "705704d9-d8ec-4dc4-da88-016d37d8b1bb"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\r\n",
        "\r\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\r\n",
        "\r\n",
        "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb], epochs=10)\r\n",
        "\r\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.6635 - val_loss: 0.7135\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5899 - val_loss: 0.6581\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5495 - val_loss: 0.6276\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.6031\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4963 - val_loss: 0.5841\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4780 - val_loss: 0.5670\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4633 - val_loss: 0.5549\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4512 - val_loss: 0.5486\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.5379\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.5350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuohX6NQv1Ho",
        "outputId": "b9e6d6c2-66e3-4e29-8fd0-9fb8d1cea2f8"
      },
      "source": [
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 796us/step - loss: 0.5236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9S7iAuKwYwP"
      },
      "source": [
        "### Early Stopping and callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n0YSo7VwfOa"
      },
      "source": [
        "To implement Early stopping simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. You can combine both callbacks to save checkpoints of your model (in case your computer crashes) and interrupt training early when there is no more progress (to avoid wasting time and resources):\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YES0IR7sv1t8",
        "outputId": "e015a9fb-d55d-42fb-84fa-6eedabc17ad7"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\r\n",
        "\r\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.5279\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.5213\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.5168\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.5160\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.5099\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.5106\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.5047\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.5032\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.5052\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4984\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.4945\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.4879\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.4890\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4919\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4957\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.4860\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.4907\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.4853\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4899\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.4838\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3642 - val_loss: 0.4838\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.4885\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3616 - val_loss: 0.4933\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3601 - val_loss: 0.4831\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.4881\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.4870\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3571 - val_loss: 0.4793\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.4845\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.4869\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.4824\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3533 - val_loss: 0.4812\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.4762\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.4833\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.4833\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.4785\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.4812\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.4883\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.4826\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.4846\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3465 - val_loss: 0.4814\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.4868\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.4825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RBhQHsAzlQO"
      },
      "source": [
        "The number of epochs can be set to a large value since training will stop automati‐ cally when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1mG38ED0OX3"
      },
      "source": [
        "### Writing custom callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siieN1tLxM3W"
      },
      "source": [
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs):\r\n",
        "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE7Tn14G0rpe",
        "outputId": "00cdea48-aa7f-4f87-ec34-f70226c74178"
      },
      "source": [
        "val_train_ratio_cb = PrintValTrainRatioCallback()\r\n",
        "history = model.fit(X_train, y_train, epochs=1,\r\n",
        "                    validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[val_train_ratio_cb])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "360/363 [============================>.] - ETA: 0s - loss: 0.3516\n",
            "val/train: 1.38\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.4840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97jtmxT0AQy"
      },
      "source": [
        "#### More on callbacks: \r\n",
        "- [Keras docs](https://keras.io/api/callbacks/)\r\n",
        "- [TensorFlow docs](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)"
      ]
    }
  ]
}
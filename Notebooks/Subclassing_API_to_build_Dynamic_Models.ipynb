{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subclassing-API-to-build-Dynamic-Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRqUxpgicsaZ"
      },
      "source": [
        "# The SubClassing API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAjNo2dhcB6r"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHQfOtTGcW3N",
        "outputId": "16fe4e17-ba49-4c98-97e1-ec1d97973009"
      },
      "source": [
        "# loading and splitting the dataset into train, valid and test set\r\n",
        "housing = fetch_california_housing()\r\n",
        "\r\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO9ULPrCcXW7"
      },
      "source": [
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_valid = scaler.fit_transform(X_valid)\r\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2me4qJpmeuzV"
      },
      "source": [
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\r\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\r\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\r\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ng0cFCcbgK"
      },
      "source": [
        "- Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference.\r\n",
        "\r\n",
        "- This has many advantages: the model can easily be saved, cloned, and shared; its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly easy to debug, since the whole model is a static graph of layers. \r\n",
        "\r\n",
        "- But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you. \r\n",
        "\r\n",
        "- Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. You can then compile it, evaluate it, and use it to make predictions, exactly like we just did [here](https://github.com/Gladiator07/Deep-Learning/blob/main/Notebooks/Functional_API.ipynb): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ240_HTcZQ8"
      },
      "source": [
        "class WideAndDeepModel(keras.Model):\r\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\r\n",
        "        super().__init__(**kwargs) # handles standard args (e.g., name)\r\n",
        "        self.hidden1 = keras.layers.Dense(units, activation=activation)\r\n",
        "        self.hidden2 = keras.layers.Dense(units, activation=activation)\r\n",
        "        self.main_output = keras.layers.Dense(1)\r\n",
        "        self.aux_output = keras.layers.Dense(1)\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        input_A, input_B = inputs\r\n",
        "        hidden1 = self.hidden1(input_B)\r\n",
        "        hidden2 = self.hidden2(hidden1)\r\n",
        "        concat = keras.layers.concatenate([input_A, hidden2])\r\n",
        "        main_output = self.main_output(concat)\r\n",
        "        aux_output = self.aux_output(hidden2)\r\n",
        "        return main_output, aux_output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmfQaYOIeMCA"
      },
      "source": [
        "model = WideAndDeepModel(30, activation=\"relu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxvSaREHeqdT",
        "outputId": "54a82c56-2124-44aa-c9d3-26d6ceb1beb5"
      },
      "source": [
        "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\r\n",
        "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\r\n",
        "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\r\n",
        "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\r\n",
        "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 2.6147 - output_1_loss: 2.5023 - output_2_loss: 3.6257 - val_loss: 1.2881 - val_output_1_loss: 1.1049 - val_output_2_loss: 2.9370\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 1.0461 - output_1_loss: 0.8862 - output_2_loss: 2.4855 - val_loss: 0.9388 - val_output_1_loss: 0.8090 - val_output_2_loss: 2.1071\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.8388 - output_1_loss: 0.7178 - output_2_loss: 1.9275 - val_loss: 0.8228 - val_output_1_loss: 0.7198 - val_output_2_loss: 1.7494\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.7572 - output_1_loss: 0.6548 - output_2_loss: 1.6782 - val_loss: 0.7656 - val_output_1_loss: 0.6753 - val_output_2_loss: 1.5790\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.7134 - output_1_loss: 0.6206 - output_2_loss: 1.5484 - val_loss: 0.7287 - val_output_1_loss: 0.6448 - val_output_2_loss: 1.4835\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.6842 - output_1_loss: 0.5971 - output_2_loss: 1.4685 - val_loss: 0.7029 - val_output_1_loss: 0.6234 - val_output_2_loss: 1.4187\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.6620 - output_1_loss: 0.5791 - output_2_loss: 1.4084 - val_loss: 0.6814 - val_output_1_loss: 0.6052 - val_output_2_loss: 1.3673\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.6437 - output_1_loss: 0.5643 - output_2_loss: 1.3582 - val_loss: 0.6628 - val_output_1_loss: 0.5895 - val_output_2_loss: 1.3227\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.6279 - output_1_loss: 0.5514 - output_2_loss: 1.3158 - val_loss: 0.6483 - val_output_1_loss: 0.5778 - val_output_2_loss: 1.2826\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.6143 - output_1_loss: 0.5408 - output_2_loss: 1.2756 - val_loss: 0.6338 - val_output_1_loss: 0.5658 - val_output_2_loss: 1.2450\n",
            "162/162 [==============================] - 0s 2ms/step - loss: 0.5618 - output_1_loss: 0.4891 - output_2_loss: 1.2166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asn9TqTYe24m"
      },
      "source": [
        "This example looks very much like the Functional API, except we do not need to create the inputs; we just use the input argument to the call() method, and we separate the creation of the layers21 in the constructor from their usage in the call() method. \r\n",
        "\r\n",
        "The big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations—your imagina‐ tion is the limit. This makes it a great API for researchers experi‐ menting with new ideas. \r\n",
        "\r\n",
        "This extra flexibility does come at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. \r\n",
        "\r\n",
        "Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDA4X0rFfGzL"
      },
      "source": [
        "## Saving and Restoring Model for Dynamic Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8aixAByfGH7"
      },
      "source": [
        "# model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqGGLfZ3fdMa"
      },
      "source": [
        "#### This statement will give error as, Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
      ]
    }
  ]
}
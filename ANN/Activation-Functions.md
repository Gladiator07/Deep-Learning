# Activation Functions
- [Common Activation Functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
- [Great blog](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
- [CS231n Notes](https://cs231n.github.io/neural-networks-1/)

### Variants of RELU
- [GELU, SELU, ELU, RELU](https://mlfromscratch.com/activation-functions-explained/#/) - a great article explaining in and outs of activation functions (sigmoid, RELU, ELU, SELU, GELU) 


### Papers to read
- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification - He](https://paperswithcode.com/paper/delving-deep-into-rectifiers-surpassing-human) - with implementation
- [Empirical Evaluation of Rectified Activations in Convolutional Network](https://paperswithcode.com/paper/empirical-evaluation-of-rectified-activations/review/) - with implementation
- [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://paperswithcode.com/method/elu) - with implementation
- [Self-Normalizing Neural Networks](https://paperswithcode.com/paper/self-normalizing-neural-networks) - with implementation